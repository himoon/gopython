# Chapter 13 생성형 AI 기사 번역 앱

## 📋 실습 개요
이번 장에서는 **생성형 AI와 웹 스크래핑**을 결합하여 뉴스 기사를 자동으로 번역하는 웹 애플리케이션을 구현합니다.
- **Ollama**를 활용한 로컬 LLM(Large Language Model) 환경 구축
- **Trafilatura**를 통한 웹 페이지 본문 추출 및 구조화
- **Streamlit**으로 사용자 친화적인 번역 인터페이스 구현
- **실시간 스트리밍**과 **다중 모델 비교** 기능을 갖춘 챗봇 구현

## ⚙️ 패키지 설치
실습을 원활하게 진행하기 위해 비주얼 스튜디오 코드 터미널에서 아래 명령어를 실행하여 필요한 파이썬 패키지들을 설치해주세요.

```shell
pip install -U ollama streamlit trafilatura
```

## 🤖 AI 모델 준비하기
이 실습에서는 **Ollama**를 통해 로컬에서 실행되는 생성형 AI 모델을 사용합니다. 시스템 사양에 따라 적합한 모델을 선택하세요.

**모델 다운로드 및 설치**
1. [Ollama 공식 사이트](https://ollama.ai/)에서 프로그램 설치
2. 터미널에서 다음 명령어로 AI 모델을 다운로드하세요:

**✅ 권장 모델 옵션**
- **빠른 속도, 적은 메모리**: `ollama run gemma3:4b` (최신 권장)
- **높은 품질, 더 많은 메모리**: `ollama run gemma3:12b` (고성능 옵션)
- **다중 모델 비교용**: `ollama run llama3.1:8b` (추가 모델)

**💡 모델 선택 가이드**
- **메모리 8GB 이하**: `gemma3:4b` 추천
- **메모리 16GB 이상**: `gemma3:12b` 추천  
- **다중 모델 실습**: `gemma3:4b` + `llama3.1:8b` 조합

> 📋 **추천**: 기존 코드에서 `gemma2:9b`를 사용하는 경우, `gemma3:4b` 또는 `gemma3:12b`로 변경하여 사용하세요!

## 🚀 실습 단계별 가이드
*   **[step_1_1.py](step_1_1.py)**: 실습에 필요한 `input` 폴더를 생성하여 기본 작업 환경을 구성합니다.

*   **[step_1_2.py](step_1_2.py)**: `ollama` 라이브러리를 사용하여 대화형 AI 모델과 기본적인 상호작용을 시도하지만, 이전 대화 내용을 기억하지 못하는 stateless 특성을 확인합니다.

*   **[step_1_3.py](step_1_3.py)**: 대화 기록을 리스트에 저장하고 매번 요청 시 함께 전송하여, 모델이 이전 대화 내용을 기억하고 연속적인 대화(stateful)를 할 수 있도록 구현합니다.

*   **[step_2_1.py](step_2_1.py)**: `streamlit`과 `st.session_state`를 활용하여 대화 기록을 웹 세션에 저장하고 시각적으로 표시하는 기본적인 챗봇 웹 애플리케이션을 만듭니다.

*   **[step_2_2_zip.py](step_2_2_zip.py)**: 파이썬 내장 함수 `zip`을 사용하여 여러 개의 리스트를 병렬로 묶어 처리하는 방법을 학습하는 기본 예제입니다.

*   **[step_2_2.py](step_2_2.py)**: `streamlit`의 열(column) 기능을 활용하여 두 개의 다른 언어 모델(Gemma, Llama)의 답변을 나란히 비교하며 보여주는 모델 비교 챗봇 앱을 구현합니다.

*   **[step_3_1_trafilatura.py](step_3_1_trafilatura.py)**: `trafilatura` 라이브러리를 사용하여 웹 페이지(URL)에서 제목, 작성자, 발행일 등 메타데이터를 포함한 구조화된 데이터(JSON)를 추출하는 방법을 학습합니다.

*   **[step_3_1.py](step_3_1.py)**: `trafilatura`를 사용하여 지정된 URL의 뉴스 기사에서 본문 텍스트(마크다운 형식)와 대표 이미지 URL을 정확하게 추출합니다.

*   **[step_3_2_sys_prompt.py](step_3_2_sys_prompt.py)**: `ollama.chat` 함수에 시스템 프롬프트(system prompt)를 전달하여 AI 모델의 역할이나 답변 스타일을 구체적으로 지정하는 방법을 학습하는 기본 예제입니다.

*   **[step_3_2.py](step_3_2.py)**: `trafilatura`로 추출한 기사 본문을 `ollama` 모델에 전문 번역가 역할의 시스템 프롬프트와 함께 전달하여, 고품질 번역 작업을 수행하도록 구현합니다.

*   **[step_3_3.py](step_3_3.py)**: 사용자가 URL을 입력하면 기사 원문과 이미지를 보여주고, AI가 번역한 결과를 나란히 표시하는 완성된 형태의 `streamlit` 기사 번역 웹 애플리케이션을 구현합니다.

*   **[step_x.py](step_x.py)**: `streamlit`의 `st.write_stream`을 사용하여 모델의 답변을 실시간 스트리밍 형태로 표시하고, 답변 생성 중에는 입력창을 비활성화하여 사용자 경험을 개선한 고급 챗봇 앱을 구현합니다.

모든 준비가 완료되었다면 생성형 AI 기사 번역 앱 실습을 시작해보세요! 🚀
